{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "878ad3db-4014-423f-bb10-314d5f4dd6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, PreTrainedTokenizerFast, BartForConditionalGeneration\n",
    "from datasets import Dataset, load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a6bcff4-5af1-453f-86e3-c78e0a3dcf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be11609-14ff-480a-9415-44c27f3ef415",
   "metadata": {},
   "source": [
    "## 데이터 로드\n",
    "### 삼성전자 관련된 하나의 토픽에 대한 30개 기사(혜진님이 슬랙에 올려주신 파일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f104bbd0-932f-4958-abf1-d5e6d99fd877",
   "metadata": {},
   "outputs": [],
   "source": [
    "article30 = pd.read_csv('result30.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9864f4d2-a3c9-4d88-9f45-d57a2f761d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text = text.replace('<b>', '')\n",
    "    text = text.replace('</b>', '')\n",
    "    text = text.replace('&apos;', '')\n",
    "    text = text.replace('...', '')\n",
    "    text = text.replace('\\u200b', '')\n",
    "    text = text.replace('M&amp;A', '')\n",
    "    text = text.replace('&quot;', '')\n",
    "    text = text.replace('\\xa0', '')\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2dccc0b2-d514-4f1f-85fb-7a0cea02dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_title_context = []\n",
    "for _, t in article30.iterrows():\n",
    "    tem = [c for c in t['context'].split('.') if len(c) > 3]\n",
    "    try:\n",
    "        con = t['title'] + ' ' + tem[0] + '. ' + tem[1] + '.'\n",
    "    except:\n",
    "        con = t['title'] + ' ' + tem[0] + '.'\n",
    "    con = preprocessing(con)\n",
    "    add_title_context.append(con)\n",
    "\n",
    "title_context = [preprocessing(con) for con in article30.title]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f0515f-b712-4aa4-bdc4-b2938235b033",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test 1\n",
    "### 제목 + 본문 앞 2문장(description말고 context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c83ebb-5495-417a-9400-8cafda056d6a",
   "metadata": {},
   "source": [
    "30개 기사를 (제목 + 앞 1,2문장) 합친 후, 30개 내용을 하나로 합쳐서 요약."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0fbc27-66cd-4350-9ef4-6f6b5152fed3",
   "metadata": {},
   "source": [
    "### KoT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f95e7271-1c81-44e2-98e2-b8b47d2cf4c7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(50358, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(50358, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(50358, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50358, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KoT5_tokenizer = AutoTokenizer.from_pretrained(\"noahkim/KoT5_news_summarization\")\n",
    "KoT5 = AutoModelForSeq2SeqLM.from_pretrained(\"noahkim/KoT5_news_summarization\")\n",
    "KoT5.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d702d439-c746-4c97-a8fd-a6818df8062b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids length:  4002\n",
      "삼성전자는 스마트폰으로 동영상 촬영 시 전방위서 들려오는 소리를 녹음할 수 있도록 갤럭시 버즈2 프로 및 갤럭시 Z 플립4·폴드4의 소프트웨어 업데이트를 13일부터 시행하여 스마트폰 카메라의 사용성을 한 단계 더 높이고 스마트폰 동영상 촬영 시 360도 사운드 녹음이 가능하도록 갤럭시 버즈2 프로와 갤럭시 Z 플립4·폴드4의 소프트웨어 업데이트를 통해 360 오디오 레코딩 기능을 지원한다.\n",
      "4.22922682762146\n"
     ]
    }
   ],
   "source": [
    "# 합친 내용이 너무 긴 경우에는 에러 발생할 수 있음\n",
    "s = time.time()\n",
    "input_ids = KoT5_tokenizer.encode('.'.join(add_title_context))\n",
    "print('input_ids length: ', len(input_ids))\n",
    "summary_ids = KoT5.generate(torch.tensor([input_ids]).to(device),  num_beams=4,  max_length=512,  eos_token_id=1)\n",
    "print(KoT5_tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True))\n",
    "print(time.time() - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717aa671-01ed-4de8-9838-65b093486de7",
   "metadata": {},
   "source": [
    "- 요약문 품질은 나쁘지 않지만, 한 문장에 비해 너무 길게 출력되고, 시간이 오래 걸림."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a054531b-bac1-4a82-9602-c1a5513f2909",
   "metadata": {},
   "source": [
    "## Kobart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "486fecb1-4736-43d6-9fbd-bce2250efbb2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(30000, 768, padding_idx=3)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Kobart_tokenizer = PreTrainedTokenizerFast.from_pretrained('digit82/kobart-summarization')\n",
    "Kobart = BartForConditionalGeneration.from_pretrained('digit82/kobart-summarization')\n",
    "Kobart.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c5ed5fa-dc8c-4895-8fff-9620f3eb0bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids length:  2693\n",
      "삼성전자는 13 스마트폰으로 동영상 촬영 시 전방위서 들려오는 소리를 녹음할 수 있도록 갤럭시 버즈2 프로 및 갤럭시 Z 플립4·폴드4의 소프트웨어 업데이트를 13일부터 시행한다고 밝혔다.\n",
      "0.4527285099029541\n"
     ]
    }
   ],
   "source": [
    "# input_ids 길이가 1020개 쯔음 이상부터는 에러 발생\n",
    "\n",
    "s = time.time()\n",
    "input_ids = Kobart_tokenizer.encode('.'.join(add_title_context))\n",
    "print('input_ids length: ', len(input_ids))\n",
    "summary_ids = Kobart.generate(torch.tensor([input_ids[:1020]]).to(device),  num_beams=4,  max_length=512,  eos_token_id=1)\n",
    "print(Kobart_tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True))\n",
    "print(time.time() - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ace500-b392-4cab-b7b1-b2fbcbddf58d",
   "metadata": {},
   "source": [
    "- KoT5보다 짧은 출력이라 사용하기 좋고, 속도도 빠름. 인풋 길이가 길면 에러나는 단점 때문에 여러 요약 단계를 거치거나, 앞부분 기사 내용만 반영해야 하는 단점."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51458bf2-b531-4788-8163-0f3f36881f86",
   "metadata": {},
   "source": [
    "## Test 2\n",
    "### 기사 제목만 가져다가 요약 생성한 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ce7e16-e1b1-4adb-991b-bb6a73c67691",
   "metadata": {},
   "source": [
    "### KoT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3684808f-e2b9-4948-9004-0bd8cc947897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids length:  775\n",
      "삼성전자, 갤럭시 버즈2 프로·워치에 소프트웨어 업데이트.삼성전자, 갤럭시 버즈2 프로·워치로 스마트폰 카메라 사용성 한 차원 상향.삼성전자, 갤럭시 버즈2 프로·워치로 스마트폰 카메라 사용성 한 차원 상향.삼성전자, 갤럭시 버즈2 프로·워치로 스마트폰 카메라 사용성 한 차원 상향.삼성전자, 갤럭시 버즈2 프로·워치로 스마트폰 카메라 사용성 한 차원 상향.삼성전자, 갤럭시 버즈2 프로·워치로 스마트폰 카메라 사용성 한 차원 상향.삼성전자, 갤럭시 버즈2 프로·워치로 스마트폰 카메라 사용성 한 차원 상향.삼성전자, 갤럭시 버즈2 프로·워치로 스마트폰 카메라 사용성 한 차원 상향.삼성전자, 갤럭시 버즈2 프로·워치로 스마트폰 카메라 사용성 한 차원 상향.삼성전자, 갤럭시 버즈2 프로·워치로 스마트폰 카메라 사용성 한 차원 상향.삼성전자, 갤럭시 버즈2 프로·워치로 스마트폰 카메라 사용성 한 차원 상향.삼성전자, 갤럭시 버즈2 프로·워치로 스마트폰 카메라 사용성 한 차원 상향.삼성전자, 갤럭시 버즈2 프로·워치로 스마트폰 카메라 사용성 한 차원 상향.삼성전자, 갤럭시 버즈2 프로·워치로 스마트폰 카메라 사용성 한 차원 상향.삼성전자, 갤럭시 버즈2 프로·워치로 스마트폰 카메라 사용성 한 차원 상향.삼성전자, 갤럭시 버즈2 프로·워치로 스마트폰 카메라 사용성 한 차원 상향.삼성전자, 갤럭시 버즈2 프로·워치로 스마트폰 카메라 사용성 한 차원 상향.삼성전자, 갤럭시 버즈2 프로·워치로 스마트폰 카메라 사용성 한 차원 상향\n",
      "11.093422412872314\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "input_ids = KoT5_tokenizer.encode('.'.join(title_context))\n",
    "print('input_ids length: ', len(input_ids))\n",
    "summary_ids = KoT5.generate(torch.tensor([input_ids]).to(device),  num_beams=4,  max_length=512,  eos_token_id=1)\n",
    "print(KoT5_tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True))\n",
    "print(time.time() - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4408ce35-24ba-4f49-bff8-53e4b7624d84",
   "metadata": {},
   "source": [
    "- 기사 내용 더했을 때 보다 더 오래걸리고 문장도 이상함. 이유는 ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6a5d81-b9d4-4f6c-819d-5b694df849e1",
   "metadata": {},
   "source": [
    "### Kobart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abf7cd22-71d3-48b0-a18d-33896eabfd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids length:  570\n",
      "삼성전자 버 버즈2프로 SW 업데이트, 버즈2프로 SW 업데이트, 버즈2프로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 스마트폰 카메라 사용성 강화, 버즈2프로 SW 업데이트, 버즈2프로 SW 업데이트, 버즈2프로 SW 업데이트, 버즈2프로 SW 업데이트, 버즈2프로 줌 인, 워치로 스마트폰 카메라 사용성 강화, 버즈2프로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 스마트폰 카메라 사용성 강화, 버즈2프로 SW 업데이트, 버즈2프로 SW 업데이트, 버즈2프로 SW 업데이트, 버즈2프로 줌 인, 워치로 스마트폰 카메라 사용성 강화, 버즈2프로 줌 인, 워치로 줌 인, 360도 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 스마트폰 카메라 사용성 강화, 버즈2프로 SW 업데이트, 버즈2프로 줌 인, 워치로 줌 인, 워치로 스마트폰 카메라 사용성 강화, 버즈2프로 SW 업데이트, 버즈2프로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인, 워치로 줌 인 등 버즈2프로 SW 업데이트 폰카로 360도 사운드\n",
      "3.9769628047943115\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "input_ids = Kobart_tokenizer.encode('.'.join(title_context))\n",
    "print('input_ids length: ', len(input_ids))\n",
    "summary_ids = Kobart.generate(torch.tensor([input_ids[:1020]]).to(device),  num_beams=4,  max_length=512,  eos_token_id=1)\n",
    "print(Kobart_tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True))\n",
    "print(time.time() - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34fc9a7-efbc-4980-8d96-9016a585b042",
   "metadata": {},
   "source": [
    "- KoT5랑 비슷하게 시간도 오래걸리고 요약문도 멍청하게 나옴."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da6319f-536e-4a1f-983c-0f349b22e066",
   "metadata": {},
   "source": [
    "## Test 3\n",
    "### Naver Clova 기사 요약 모델 API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80492d22-d1ab-479d-9a08-e7ec90501cfe",
   "metadata": {},
   "source": [
    "- API 신청: https://www.ncloud.com/product/aiService/clovaSummary\n",
    "- python 코드 예시: https://blog.naver.com/n_cloudplatform/222485043721"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9adedd8-4f27-46d9-ad05-7db5fa0cef1d",
   "metadata": {},
   "source": [
    "### 제목 + 앞 1,2문장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf3014eb-8052-4527-aa0f-60d3119029b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43576884269714355\n",
      "{\"summary\":\"삼성전자, 버즈2프로 SW 업데이트…360도 소리 녹음 지원 = 삼성전자는 스마트폰으로 동영상 촬영 시 전방위서 들려오는 소리를 녹음할 수 있도록 갤럭시 버즈2 프로 및 갤럭시 Z 플립4·폴드4의 소프트웨어 업데이트를 13일부터 시행한다고 밝혔습니다.\\n스마트폰 동영상 촬영 시 전방위서 들려오는 소리를 녹음할 수 있게 하기 위해서다..삼성전자, 버즈2프로 SW 업데이트…360도 소리 녹음 지원 삼성전자가 스마트폰으로 동영상 촬영 시 전방위서 들려오는 소리를 녹음할 수 있도록 갤럭시 버즈2 프로 및 갤럭시 Z 플립4·폴드4의 소프트웨어 업데이트를 13일부터 시행합니다.\\n삼성전자 버즈2프로 업데이트···\\\"360도 소리 녹음 지원\\\" 삼성전자는 스마트폰으로 동영상 촬영 시 전방위에서 들려오는 소리를 녹음할 수 있도록 갤럭시 버즈2 프로 및 갤럭시 Z 플립4·폴드4의 소프트웨어 업데이트를 시행한다고 13일 밝혔습니다.\"}\n"
     ]
    }
   ],
   "source": [
    "client_id = \"px9opb5zus\"\n",
    "client_secret = \"vBCjpPifA3fmQipwmHs6OS7eBw1D8RsZy6iHC96y\"\n",
    "\n",
    "s = time.time()\n",
    "\n",
    "headers = {\n",
    "    \"X-NCP-APIGW-API-KEY-ID\": client_id,\n",
    "    \"X-NCP-APIGW-API-KEY\": client_secret,\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "language = \"ko\" # Language of document (ko, ja )\n",
    "model = \"news\" # Model used for summaries (general, news)\n",
    "tone = \"2\" # Converts the tone of the summarized result. (0, 1, 2, 3)\n",
    "summaryCount = \"3\" # This is the number of sentences for the summarized document.\n",
    "url= \"https://naveropenapi.apigw.ntruss.com/text-summary/v1/summarize\" \n",
    "title= \"'하루 2000억' 판 커지는 간편송금 시장\"\n",
    "content = '.'.join(add_title_context)[:1500] # 입력 문장이 너무 길면 에러 발생\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"document\": {\n",
    "    \"title\": title,\n",
    "    \"content\" : content\n",
    "    },\n",
    "    \"option\": {\n",
    "    \"language\": language,\n",
    "    \"model\": model,\n",
    "    \"tone\": tone,\n",
    "    \"summaryCount\" : summaryCount\n",
    "    }\n",
    "}\n",
    "\n",
    "# print(json.dumps(data, indent=4, sort_keys=True))\n",
    "response = requests.post(url, data=json.dumps(data), headers=headers)\n",
    "rescode = response.status_code\n",
    "\n",
    "print(time.time() - s)\n",
    "if(rescode == 200):\n",
    "    print (response.text)\n",
    "else:\n",
    "    print(\"Error : \" + response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d024f602-40dc-4c52-a903-a1fb231f9e7d",
   "metadata": {},
   "source": [
    "- 생성할 요약 문장 개수 지정 가능하고 요약 문장 길이도 짧게 잘 나옴.\n",
    "- 단점은 높임말로 문장이 생성되어 어색할 수 있다는 점.\n",
    "- url이랑 title도 같이 넘겨주는데, url은 왜 같이 넘겨주는지 모르겠음.\n",
    "- 1000회 이하 호출시 -> 무료   //   1000회 초과 호출시 -> 호출당 2원"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bcb10e-5028-4a90-9c70-31121801166a",
   "metadata": {},
   "source": [
    "### 제목만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b3e50e4-8e1f-44b1-91b9-04b0473e3431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33722472190856934\n",
      "{\"summary\":\"삼성 SW 업데이트.버즈로 360도 녹음, 워치로 줌 인…삼성, 폰카 편의성 높입니다.\\n갤럭시 버즈2 프로·워치 등 업데이트, 삼성전자 스마트폰 카메라 사용성 차.삼성전자, 버즈2 프로·워치로 스마트폰 카메라 사용성 한 차원 상향.삼성전자, 버즈2프로 SW 업데이트360도 소리 녹음 지원.삼성전자, 갤럭시버즈2프로 SW 업데이트…360도 소리 녹음 지원.360도 사운드 녹음 삼성전자, 갤럭시 버즈2 프로·워치 SW 업데이트.삼성, 갤버즈2 프로로 플립4·폴드4 360도 녹음 지원.워치로 폰카 줌·버즈로 360도 녹음…삼성전자, 업데이트 진행.녹음되는 갤버즈 vs 보급형 에어팟…삼성-애플, 무선이어폰 대전.버즈로 360도 녹음 '삼성 폰카'의 진화.삼성전자, 버즈2 프로 SW 업데이트 폰카로 360도 사운드 담습니다.\\n삼성전자, 갤럭시 버즈2 프로·워치에 소프트웨어 업데이트.삼성전자, 버즈2 프로 SW업데이트360 오디오 레코딩 기능 지원.삼성전자, 버즈2프로·갤워치로 스마트폰 카메라 사용성 높입니다.\"}\n"
     ]
    }
   ],
   "source": [
    "client_id = \"px9opb5zus\"\n",
    "client_secret = \"vBCjpPifA3fmQipwmHs6OS7eBw1D8RsZy6iHC96y\"\n",
    "\n",
    "s = time.time()\n",
    "\n",
    "headers = {\n",
    "    \"X-NCP-APIGW-API-KEY-ID\": client_id,\n",
    "    \"X-NCP-APIGW-API-KEY\": client_secret,\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "language = \"ko\" # Language of document (ko, ja )\n",
    "model = \"news\" # Model used for summaries (general, news)\n",
    "tone = \"2\" # Converts the tone of the summarized result. (0, 1, 2, 3)\n",
    "summaryCount = \"3\" # This is the number of sentences for the summarized document.\n",
    "url= \"https://naveropenapi.apigw.ntruss.com/text-summary/v1/summarize\" \n",
    "title= \"'하루 2000억' 판 커지는 간편송금 시장\"\n",
    "content = '.'.join(title_context)[:1500] # 입력 문장이 너무 길면 에러 발생\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"document\": {\n",
    "    \"title\": title,\n",
    "    \"content\" : content\n",
    "    },\n",
    "    \"option\": {\n",
    "    \"language\": language,\n",
    "    \"model\": model,\n",
    "    \"tone\": tone,\n",
    "    \"summaryCount\" : summaryCount\n",
    "    }\n",
    "}\n",
    "\n",
    "# print(json.dumps(data, indent=4, sort_keys=True))\n",
    "response = requests.post(url, data=json.dumps(data), headers=headers)\n",
    "rescode = response.status_code\n",
    "\n",
    "print(time.time() - s)\n",
    "if(rescode == 200):\n",
    "    print (response.text)\n",
    "else:\n",
    "    print(\"Error : \" + response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83b0bec-672e-4008-89d4-464e9f3d055c",
   "metadata": {},
   "source": [
    "- 기사 내용이 포함되지 않아 구체적인 내용은 담기지 않음.\n",
    "- 쪼끔 더 빠름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0527b599-715e-4384-b059-6425d0a1f4f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
